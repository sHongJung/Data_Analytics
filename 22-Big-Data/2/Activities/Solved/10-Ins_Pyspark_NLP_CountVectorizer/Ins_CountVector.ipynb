{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('vectorizer').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sample dataframe\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"many many some few many\"),\n",
    "    (1, \"some many some few some\"),\n",
    "    (2, \"one many\")\n",
    "], [\"id\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|               words|              tokens|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|many many some fe...|[many, many, some...|\n",
      "|  1|some many some fe...|[some, many, some...|\n",
      "|  2|            one many|         [one, many]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataframe\n",
    "tokenizer = Tokenizer(inputCol=\"words\", outputCol=\"tokens\")\n",
    "tokenized = tokenizer.transform(df)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-----------------------------+---------------------------------+\n",
      "|id |words                  |tokens                       |Vectors, [Indexes], [Frequencies]|\n",
      "+---+-----------------------+-----------------------------+---------------------------------+\n",
      "|0  |many many some few many|[many, many, some, few, many]|(4,[0,1,2],[3.0,1.0,1.0])        |\n",
      "|1  |some many some few some|[some, many, some, few, some]|(4,[0,1,2],[1.0,3.0,1.0])        |\n",
      "|2  |one many               |[one, many]                  |(4,[0,3],[1.0,1.0])              |\n",
      "+---+-----------------------+-----------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the term frequency\n",
    "countvectorizer = CountVectorizer(minTF=1.0, minDF=1.0, vocabSize=20,\n",
    "                                 inputCol='tokens', outputCol='Vectors, [Indexes], [Frequencies]')\n",
    "\n",
    "model = countvectorizer.fit(tokenized)\n",
    "result = model.transform(tokenized)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Vectors\n",
    "To double check that words are matching up with the correct index we will check to see what the index of each word is in an easy to read format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|terms|\n",
      "+-----+\n",
      "| many|\n",
      "| many|\n",
      "| some|\n",
      "|  few|\n",
      "| many|\n",
      "| some|\n",
      "| many|\n",
      "| some|\n",
      "|  few|\n",
      "| some|\n",
      "|  one|\n",
      "| many|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From the tokenized dataframe grab every word used. \n",
    "from pyspark.sql.types import StringType\n",
    "df_vocab = tokenized.select('tokens').rdd.\\\n",
    "            flatMap(lambda x: x[0]).\\\n",
    "            toDF(schema=StringType()).toDF('terms')\n",
    "df_vocab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use string indexer to of each word\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringindexer = StringIndexer(inputCol='terms', outputCol='StringIndexer(index)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|terms|StringIndexer(index)|\n",
      "+-----+--------------------+\n",
      "| many|                 0.0|\n",
      "| some|                 1.0|\n",
      "|  few|                 2.0|\n",
      "|  one|                 3.0|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the string indexer to the vocab set and remove duplicates\n",
    "stringindexer.fit(df_vocab).transform(df_vocab).\\\n",
    "    distinct().\\\n",
    "    orderBy('StringIndexer(index)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
