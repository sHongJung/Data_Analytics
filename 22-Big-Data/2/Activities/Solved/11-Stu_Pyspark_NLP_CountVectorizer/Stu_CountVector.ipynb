{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName('counterVector').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      Airline Tweets|\n",
      "+--------------------+\n",
      "|@VirginAmerica pl...|\n",
      "|@VirginAmerica se...|\n",
      "|@VirginAmerica do...|\n",
      "|@VirginAmerica Ar...|\n",
      "|@VirginAmerica aw...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in csv\n",
    "dataframe = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airlines.csv\")\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      Airline Tweets|               words|\n",
      "+--------------------+--------------------+\n",
      "|@VirginAmerica pl...|[@virginamerica, ...|\n",
      "|@VirginAmerica se...|[@virginamerica, ...|\n",
      "|@VirginAmerica do...|[@virginamerica, ...|\n",
      "|@VirginAmerica Ar...|[@virginamerica, ...|\n",
      "|@VirginAmerica aw...|[@virginamerica, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataframe\n",
    "tokened = Tokenizer(inputCol=\"Airline Tweets\", outputCol=\"words\")\n",
    "tokened_transformed = tokened.transform(dataframe)\n",
    "tokened_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|      Airline Tweets|               words|            filtered|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|@VirginAmerica pl...|[@virginamerica, ...|[plus, you've, ad...|\n",
      "|@VirginAmerica se...|[@virginamerica, ...|[seriously, would...|\n",
      "|@VirginAmerica do...|[@virginamerica, ...|[do, you, miss, m...|\n",
      "|@VirginAmerica Ar...|[@virginamerica, ...|[are, the, hours,...|\n",
      "|@VirginAmerica aw...|[@virginamerica, ...|[awaiting, my, re...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "stop_list = [\"@VirginAmerica\", \"$30\", \"@virginamerica\"]\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stop_list)\n",
    "removed_frame = remover.transform(tokened_transformed)\n",
    "removed_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|      Airline Tweets|               words|            filtered|             Vectors|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|@VirginAmerica pl...|[@virginamerica, ...|[plus, you've, ad...|(20,[0,3,9,14],[1...|\n",
      "|@VirginAmerica se...|[@virginamerica, ...|[seriously, would...|(20,[0,1,4,5,7,8,...|\n",
      "|@VirginAmerica do...|[@virginamerica, ...|[do, you, miss, m...|     (20,[13],[1.0])|\n",
      "|@VirginAmerica Ar...|[@virginamerica, ...|[are, the, hours,...|(20,[0,1,2,5,6,16...|\n",
      "|@VirginAmerica aw...|[@virginamerica, ...|[awaiting, my, re...|(20,[2,3,4,10,11,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the term frequency\n",
    "countvectorizer = CountVectorizer(minTF=1.0, minDF=1.0, vocabSize=20,\n",
    "                                 inputCol='filtered', outputCol='Vectors')\n",
    "\n",
    "model = countvectorizer.fit(removed_frame)\n",
    "result = model.transform(removed_frame)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit data with the IDF\n",
    "idf = IDF(inputCol=\"Vectors\", outputCol=\"Features, [Indexes], [TF-IDF]\")\n",
    "idfModel = idf.fit(result)\n",
    "rescaledData = idfModel.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------------+\n",
      "|            filtered|Features, [Indexes], [TF-IDF]|\n",
      "+--------------------+-----------------------------+\n",
      "|[plus, you've, ad...|         (20,[0,3,9,14],[0...|\n",
      "|[seriously, would...|         (20,[0,1,4,5,7,8,...|\n",
      "|[do, you, miss, m...|         (20,[13],[1.09861...|\n",
      "|[are, the, hours,...|         (20,[0,1,2,5,6,16...|\n",
      "|[awaiting, my, re...|         (20,[2,3,4,10,11,...|\n",
      "+--------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the dataframe\n",
    "rescaledData.select(\"filtered\", \"Features, [Indexes], [TF-IDF]\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        terms|\n",
      "+-------------+\n",
      "|         plus|\n",
      "|       you've|\n",
      "|        added|\n",
      "|  commercials|\n",
      "|           to|\n",
      "|          the|\n",
      "|experience...|\n",
      "|       tacky.|\n",
      "|    seriously|\n",
      "|        would|\n",
      "|          pay|\n",
      "|            a|\n",
      "|       flight|\n",
      "|          for|\n",
      "|        seats|\n",
      "|         that|\n",
      "|       didn't|\n",
      "|         have|\n",
      "|         this|\n",
      "|     playing.|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From the tokenized dataframe grab every word used. \n",
    "from pyspark.sql.types import StringType\n",
    "df_vocab = removed_frame.select('filtered').rdd.\\\n",
    "            flatMap(lambda x: x[0]).\\\n",
    "            toDF(schema=StringType()).toDF('terms')\n",
    "df_vocab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use string indexer to of each word\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringindexer = StringIndexer(inputCol='terms', outputCol='StringIndexer(index)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   terms|StringIndexer(index)|\n",
      "+--------+--------------------+\n",
      "|     the|                 0.0|\n",
      "|     for|                 1.0|\n",
      "|     are|                 2.0|\n",
      "|   would|                 3.0|\n",
      "|  online|                 4.0|\n",
      "|    that|                 5.0|\n",
      "|      to|                 6.0|\n",
      "|    it's|                 7.0|\n",
      "|current?|                 8.0|\n",
      "|    this|                 9.0|\n",
      "|    have|                10.0|\n",
      "|    your|                11.0|\n",
      "|   worry|                12.0|\n",
      "|  didn't|                13.0|\n",
      "|    plus|                14.0|\n",
      "|  flight|                15.0|\n",
      "|awaiting|                16.0|\n",
      "|  return|                17.0|\n",
      "|      do|                18.0|\n",
      "|    club|                19.0|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the string indexer to the vocab set and remove duplicates\n",
    "stringindexer.fit(df_vocab).transform(df_vocab).\\\n",
    "    distinct().\\\n",
    "    orderBy('StringIndexer(index)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
