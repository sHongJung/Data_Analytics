{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Twitter API Keys\n",
    "consumer_key = \"Ed4RNulN1lp7AbOooHa9STCoU\"\n",
    "consumer_secret = \"P7cUJlmJZq0VaCY0Jg7COliwQqzK0qYEyUF9Y0idx4ujb3ZlW5\"\n",
    "access_token = \"839621358724198402-dzdOsx2WWHrSuBwyNUiqSEnTivHozAZ\"\n",
    "access_token_secret = \"dCZ80uNRbFDjxdU2EckmNiSckdoATach6Q8zb7YYYE5ER\"\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "# Target Search Term\n",
    "target_term = \"@CNNbrk\"\n",
    "\n",
    "# Lists to hold sentiments\n",
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "\n",
    "# Grab 25 tweets\n",
    "public_tweets = api.search(target_term, count=25, result_type=\"recent\")\n",
    "\n",
    "# Loop through all tweets\n",
    "for tweet in public_tweets[\"statuses\"]:\n",
    "\n",
    "    # Run Vader Analysis on each tweet\n",
    "    compound = analyzer.polarity_scores(tweet[\"text\"])[\"compound\"]\n",
    "    pos = analyzer.polarity_scores(tweet[\"text\"])[\"pos\"]\n",
    "    neu = analyzer.polarity_scores(tweet[\"text\"])[\"neu\"]\n",
    "    neg = analyzer.polarity_scores(tweet[\"text\"])[\"neg\"]\n",
    "\n",
    "    # Add each value to the appropriate array\n",
    "    compound_list.append(compound)\n",
    "    positive_list.append(pos)\n",
    "    negative_list.append(neg)\n",
    "    neutral_list.append(neu)\n",
    "\n",
    "# Store the Average Sentiments\n",
    "sentiment = {\"Compound\": np.mean(compound_list),\n",
    "             \"Positive\": np.mean(positive_list),\n",
    "             \"Neutral\": np.mean(negative_list),\n",
    "             \"Negative\": np.mean(neutral_list)}\n",
    "\n",
    "# Print the Sentiments\n",
    "print(sentiment)\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alternative solution\n",
    "\n",
    "# Dependencies\n",
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Twitter API Keys\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"-\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Target Search Term\n",
    "target_term = \"@CNNbrk\"\n",
    "\n",
    "# Lists to hold sentiments\n",
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "\n",
    "# Grab two and only two pages of tweets\n",
    "largest_id = None\n",
    "pages_processed = 0\n",
    "\n",
    "for page in tweepy.Cursor(api.search, q=target_term, count=100, result_type='recent', \n",
    "                          max_id=largest_id).pages(2):\n",
    "    \n",
    "    if not largest_id:\n",
    "        largest_id = page[0].id\n",
    "        \n",
    "    # Loop through all tweets\n",
    "    for t in page:\n",
    "        tweet = t._json\n",
    "        # Run Vader Analysis on each tweet\n",
    "        score = analyzer.polarity_scores(tweet[\"text\"])\n",
    "        compound = score[\"compound\"]\n",
    "        pos = score[\"pos\"]\n",
    "        neu = score[\"neu\"]\n",
    "        neg = score[\"neg\"]\n",
    "\n",
    "        # Add each value to the appropriate array\n",
    "        compound_list.append(compound)\n",
    "        positive_list.append(pos)\n",
    "        negative_list.append(neg)\n",
    "        neutral_list.append(neu)\n",
    "\n",
    "# Store the Average Sentiments\n",
    "sentiment = {\"Compound\": np.mean(compound_list),\n",
    "             \"Positive\": np.mean(positive_list),\n",
    "             \"Neutral\": np.mean(neutral_list),\n",
    "             \"Negative\": np.mean(negative_list)}\n",
    "\n",
    "# Print the Sentiments\n",
    "print(sentiment)\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
